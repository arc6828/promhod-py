{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "d:\\python\\promhod-env\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for Text 1:\n",
      "{\n",
      "    \"language\": \"th\",\n",
      "    \"entities\": [\n",
      "        {\n",
      "            \"type\": \"LOCATION\",\n",
      "            \"value\": \"ในกรุงเทพมหานคร\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"DATE\",\n",
      "            \"value\": \"18 ธันวาคม 2567\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"TIME\",\n",
      "            \"value\": \"09.00 น.\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ORGANIZATION\",\n",
      "            \"value\": \"ศูนย์ช่วยเหลือสังคม\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"PHONE\",\n",
      "            \"value\": \"1300\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"LOCATION\",\n",
      "            \"value\": \"เทพมหานคร\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"MONEY\",\n",
      "            \"value\": \"3,000 บาท600,000 บาท\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"PERSON\",\n",
      "            \"value\": \"โดยนายอนุรักษ์ มะลิวัลย์\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"LOCATION\",\n",
      "            \"value\": \"ห้องประชุมสหวิชาชีพ  ศูนย์ช่วยเหลือสังคม  ชั้น 1อาคารกรมพัฒนาสังคมและสวัสดิการ\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from langdetect import detect, DetectorFactory\n",
    "from pythainlp.wangchanberta import ThaiNameTagger\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "# Set seed for consistent language detection\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Initialize Thai NER model\n",
    "thai_ner = ThaiNameTagger()\n",
    "\n",
    "# Initialize spaCy English NER model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define mapping for common entity types\n",
    "COMMON_ENTITY_TYPES = {\n",
    "    # spaCy mappings\n",
    "    \"PERSON\": \"PERSON\",\n",
    "    \"NORP\": \"GROUP\",\n",
    "    \"FAC\": \"FACILITY\",\n",
    "    \"ORG\": \"ORGANIZATION\",\n",
    "    \"GPE\": \"LOCATION\",\n",
    "    \"LOC\": \"LOCATION\",\n",
    "    \"PRODUCT\": \"PRODUCT\",\n",
    "    \"EVENT\": \"EVENT\",\n",
    "    \"WORK_OF_ART\": \"WORK_OF_ART\",\n",
    "    \"LAW\": \"LAW\",\n",
    "    \"LANGUAGE\": \"LANGUAGE\",\n",
    "    \"DATE\": \"DATE\",\n",
    "    \"TIME\": \"TIME\",\n",
    "    \"PERCENT\": \"PERCENT\",\n",
    "    \"MONEY\": \"MONEY\",\n",
    "    \"QUANTITY\": \"QUANTITY\",\n",
    "    \"ORDINAL\": \"ORDINAL\",\n",
    "    \"CARDINAL\": \"CARDINAL\",\n",
    "\n",
    "    # WangchanBERTa mappings\n",
    "    \"DATE\": \"DATE\",\n",
    "    \"TIME\": \"TIME\",\n",
    "    \"EMAIL\": \"EMAIL\",\n",
    "    \"LEN\": \"LENGTH\",\n",
    "    \"LOCATION\": \"LOCATION\",\n",
    "    \"ORGANIZATION\": \"ORGANIZATION\",\n",
    "    \"PERSON\": \"PERSON\",\n",
    "    \"PHONE\": \"PHONE\",\n",
    "    \"URL\": \"URL\",\n",
    "    \"ZIP\": \"ZIP\",\n",
    "    \"Money\": \"MONEY\",\n",
    "    \"LAW\": \"LAW\"\n",
    "}\n",
    "\n",
    "def map_entity_type(entity_type, model):\n",
    "    \"\"\"\n",
    "    Map the entity type to a common entity type system.\n",
    "\n",
    "    Args:\n",
    "        entity_type (str): The original entity type from the model.\n",
    "        model (str): The name of the model ('spacy' or 'wangchanberta').\n",
    "\n",
    "    Returns:\n",
    "        str: The mapped entity type.\n",
    "    \"\"\"\n",
    "    return COMMON_ENTITY_TYPES.get(entity_type, \"UNKNOWN\")\n",
    "\n",
    "def unify_entities(entities, model):\n",
    "    \"\"\"\n",
    "    Convert entity types to the common type system.\n",
    "\n",
    "    Args:\n",
    "        entities (list): List of entities with 'type' and 'value'.\n",
    "        model (str): The name of the model ('spacy' or 'wangchanberta').\n",
    "\n",
    "    Returns:\n",
    "        list: Entities with mapped types.\n",
    "    \"\"\"\n",
    "    return [{\"type\": map_entity_type(entity[\"type\"], model), \"value\": entity[\"value\"]} for entity in entities]\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detect the language of the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Detected language code (e.g., 'th' for Thai, 'en' for English).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def get_named_entities_thai(text):\n",
    "    \"\"\"\n",
    "    Perform NER on Thai text and group nearby entities of the same type.\n",
    "\n",
    "    Args:\n",
    "        text (str): The Thai input text.\n",
    "\n",
    "    Returns:\n",
    "        list: Grouped named entities with spaces for DATE and PERSON types.\n",
    "    \"\"\"\n",
    "    results = thai_ner.get_ner(text)\n",
    "    grouped_entities = []\n",
    "    current_group = {\"type\": None, \"value\": \"\"}\n",
    "\n",
    "    for token, tag in results:\n",
    "        if tag == \"O\":  # Ignore tokens outside any named entity\n",
    "            continue\n",
    "\n",
    "        entity_type = tag.split('-')[-1]  # Extract type (e.g., DATE, PERSON)\n",
    "        \n",
    "        if current_group[\"type\"] == entity_type:\n",
    "            # Add space before the token for DATE and PERSON\n",
    "            if entity_type in [\"DATE\", \"PERSON\"]:\n",
    "                current_group[\"value\"] += \" \" + token\n",
    "            else:\n",
    "                current_group[\"value\"] += token\n",
    "        else:\n",
    "            if current_group[\"type\"]:\n",
    "                # Save the previous group if it exists\n",
    "                grouped_entities.append(current_group)\n",
    "            # Start a new group\n",
    "            current_group = {\"type\": entity_type, \"value\": token}\n",
    "\n",
    "    # Add the last group if it exists\n",
    "    if current_group[\"type\"]:\n",
    "        grouped_entities.append(current_group)\n",
    "\n",
    "    return [{\"type\": entity[\"type\"], \"value\": entity[\"value\"].strip()} for entity in grouped_entities]\n",
    "\n",
    "def get_named_entities_english(text):\n",
    "    \"\"\"\n",
    "    Perform NER on English text using spaCy.\n",
    "\n",
    "    Args:\n",
    "        text (str): The English input text.\n",
    "\n",
    "    Returns:\n",
    "        list: Named entities with their types and values.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [{\"type\": ent.label_, \"value\": ent.text} for ent in doc.ents]\n",
    "\n",
    "def perform_ner_based_on_language(text):\n",
    "    \"\"\"\n",
    "    Perform language detection and NER based on detected language.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the language and grouped named entities.\n",
    "    \"\"\"\n",
    "    language = detect_language(text)\n",
    "    if language == \"th\":\n",
    "        entities = get_named_entities_thai(text)\n",
    "        unified_entities = unify_entities(entities, \"wangchanberta\")\n",
    "    elif language == \"en\":\n",
    "        entities = get_named_entities_english(text)\n",
    "        unified_entities = unify_entities(entities, \"spacy\")\n",
    "    else:\n",
    "        unified_entities = []\n",
    "\n",
    "    return {\"language\": language, \"entities\": unified_entities}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Input texts\n",
    "    text1 = \"การประชุมคณะกรรมการพิจารณาให้ความช่วยเหลือผู้ประสบปัญหาทางสังคมในกรุงเทพมหานคร ครั้งที่ 10/2567 วันพุธที่ 18 ธันวาคม 2567 เวลา 09.00 น. ศูนย์ช่วยเหลือสังคม สายด่วน 1300 ดำเนินการจัดประชุมคณะกรรมการพิจารณาให้ความช่วยเหลือผู้ประสบปัญหาทางสังคมในกรุงเทพมหานคร ครั้งที่ 10/2567 เพื่อพิจารณาให้ความช่วยเหลือผู้ประสบปัญหาทางสังคมจำนวน 200 ราย รายละ 3,000 บาท รวมเป็นเงินทั้งสิ้น 600,000 บาท โดยนายอนุรักษ์ มะลิวัลย์ ผู้อำนวยการกองตรวจราชการ เป็นประธาน ณ ห้องประชุมสหวิชาชีพ ศูนย์ช่วยเหลือสังคม ชั้น 1 อาคารกรมพัฒนาสังคมและสวัสดิการ\"\n",
    "    text2 = \"The 10th Meeting of the Committee for Social Assistance in Bangkok for the Year 2567 will be held on Wednesday, December 18, 2024, at 9:00 AM. The Social Assistance Center, Hotline 1300, will organize this meeting to consider providing assistance to 200 individuals facing social problems, with each receiving 3,000 THB, totaling 600,000 THB. The meeting will be chaired by Mr. Anurak Maliwan, Director of the Inspection Division, at the Multidisciplinary Meeting Room, Social Assistance Center, 1st Floor, Department of Social Development and Welfare Building.\"\n",
    "\n",
    "    # Perform NER\n",
    "    result1 = perform_ner_based_on_language(text1)\n",
    "    # result2 = perform_ner_based_on_language(text2)\n",
    "\n",
    "    # Print results as JSON\n",
    "    print(\"Result for Text 1:\")\n",
    "    print(json.dumps(result1, ensure_ascii=False, indent=4))\n",
    "\n",
    "    # print(\"\\nResult for Text 2:\")\n",
    "    # print(json.dumps(result2, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import re\n",
    "from pythainlp.util import thai_strptime\n",
    "from datetime import datetime\n",
    "\n",
    "with open(\"news/data/provinces.json\", 'r', encoding='utf-8') as file:\n",
    "    LOCS = json.load(file)  # อ่านข้อมูล JSON และแปลงเป็น object ใน Python\n",
    "# map\n",
    "def map_function(item):    \n",
    "    return  item[\"province\"]\n",
    "\n",
    "# อาจได้ผลลัพธ์ที่มี None\n",
    "PROVINCE_LIST = list(map(map_function, LOCS))\n",
    "\n",
    "def location_detection(text):       \n",
    "    # ใช้ regex เพื่อค้นหาชื่อจังหวัด\n",
    "    province_pattern = r\"จังหวัด\\s*([^\\s|]*)|จ\\.\\s*([^\\s|]*)|เมือง\\s*([^\\s|]*)\"\n",
    "    matches = re.findall(province_pattern, text)\n",
    "\n",
    "    # ดึงชื่อจังหวัดออกมา\n",
    "    provinces = [match[0] or match[1] for match in matches]\n",
    "    unique_provinces = set(provinces)  # ลบชื่อจังหวัดที่ซ้ำกัน\n",
    "\n",
    "    # แสดงผล\n",
    "    if unique_provinces:\n",
    "        # print(f\"เหตุการณ์เกิดขึ้นที่จังหวัด: {', '.join(unique_provinces)}\")\n",
    "        clean_unique_provinces = []\n",
    "        for p in PROVINCE_LIST:\n",
    "            if p in \" | \".join(list(unique_provinces)):\n",
    "                clean_unique_provinces.append(p)\n",
    "        return clean_unique_provinces if len(clean_unique_provinces) > 0 else []\n",
    "    else:\n",
    "        # print(\"ไม่พบชื่อจังหวัดในข้อความ\")\n",
    "        return []\n",
    "\n",
    "# ฟังก์ชันแปลงวันที่ภาษาไทยเป็นรูปแบบ datetime\n",
    "def thai_date_to_standard(thai_date_str):\n",
    "    # แปลงวันที่ภาษาไทยเป็น datetime object\n",
    "    # 12 ก.ย. 2567 10:59 น.\n",
    "    date_object = thai_strptime(thai_date_str, \"%d %B %Y %H:%M น.\")\n",
    "\n",
    "    # แปลง datetime object เป็น ISO 8601 (YYYY-MM-DD)\n",
    "    return date_object.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 2762435.json as it is in news/data-detail2 folder\n",
      "Skipping 2762440.json as it is in news/data-detail2 folder\n",
      "Skipping 2770966.json as it is in news/data-detail2 folder\n",
      "Skipping 2794341.json as it is in news/data-detail2 folder\n",
      "Skipping 2797251.json as it is in news/data-detail2 folder\n",
      "Skipping 2802827.json as it is in news/data-detail2 folder\n",
      "Skipping 2803780.json as it is in news/data-detail2 folder\n",
      "Skipping 2803892.json as it is in news/data-detail2 folder\n",
      "Skipping 2804117.json as it is in news/data-detail2 folder\n",
      "Skipping 2804278.json as it is in news/data-detail2 folder\n",
      "Skipping 2804387.json as it is in news/data-detail2 folder\n",
      "Skipping 2804616.json as it is in news/data-detail2 folder\n",
      "Skipping 2805620.json as it is in news/data-detail2 folder\n",
      "Skipping 2805738.json as it is in news/data-detail2 folder\n",
      "Skipping 2805784.json as it is in news/data-detail2 folder\n",
      "Skipping 2805786.json as it is in news/data-detail2 folder\n",
      "Skipping 2806169.json as it is in news/data-detail2 folder\n",
      "Skipping 2807550.json as it is in news/data-detail2 folder\n",
      "Skipping 2809599.json as it is in news/data-detail2 folder\n",
      "Skipping 2809678.json as it is in news/data-detail2 folder\n",
      "Skipping 2809698.json as it is in news/data-detail2 folder\n",
      "Skipping 2809718.json as it is in news/data-detail2 folder\n",
      "Skipping 2809748.json as it is in news/data-detail2 folder\n",
      "Skipping 2809757.json as it is in news/data-detail2 folder\n",
      "Skipping 2809768.json as it is in news/data-detail2 folder\n",
      "Skipping 2809769.json as it is in news/data-detail2 folder\n",
      "Skipping 2809782.json as it is in news/data-detail2 folder\n",
      "Skipping 2809792.json as it is in news/data-detail2 folder\n",
      "Skipping 2809809.json as it is in news/data-detail2 folder\n",
      "Skipping 2809837.json as it is in news/data-detail2 folder\n",
      "Skipping 2809890.json as it is in news/data-detail2 folder\n",
      "Skipping 2809895.json as it is in news/data-detail2 folder\n",
      "Skipping 2809909.json as it is in news/data-detail2 folder\n",
      "Skipping 2809928.json as it is in news/data-detail2 folder\n",
      "Skipping 2809931.json as it is in news/data-detail2 folder\n",
      "Skipping 2810015.json as it is in news/data-detail2 folder\n",
      "Skipping 2810067.json as it is in news/data-detail2 folder\n",
      "Skipping 2810079.json as it is in news/data-detail2 folder\n",
      "Skipping 2810139.json as it is in news/data-detail2 folder\n",
      "Skipping 2810158.json as it is in news/data-detail2 folder\n",
      "Skipping 2810167.json as it is in news/data-detail2 folder\n",
      "Skipping 2810171.json as it is in news/data-detail2 folder\n",
      "Skipping 2810179.json as it is in news/data-detail2 folder\n",
      "Skipping 2810180.json as it is in news/data-detail2 folder\n",
      "Skipping 2810184.json as it is in news/data-detail2 folder\n",
      "Skipping 2810226.json as it is in news/data-detail2 folder\n",
      "Skipping 2810241.json as it is in news/data-detail2 folder\n",
      "Skipping 2810247.json as it is in news/data-detail2 folder\n",
      "Skipping 2810263.json as it is in news/data-detail2 folder\n",
      "Skipping 2810354.json as it is in news/data-detail2 folder\n",
      "Skipping 2810382.json as it is in news/data-detail2 folder\n",
      "Skipping 2810420.json as it is in news/data-detail2 folder\n",
      "Skipping 2810483.json as it is in news/data-detail2 folder\n",
      "Skipping 2810485.json as it is in news/data-detail2 folder\n",
      "Skipping 2810550.json as it is in news/data-detail2 folder\n",
      "Skipping 2810558.json as it is in news/data-detail2 folder\n",
      "Skipping 2810564.json as it is in news/data-detail2 folder\n",
      "Skipping 2810570.json as it is in news/data-detail2 folder\n",
      "Skipping 2810572.json as it is in news/data-detail2 folder\n",
      "Skipping 2810573.json as it is in news/data-detail2 folder\n",
      "Skipping 2810579.json as it is in news/data-detail2 folder\n",
      "Skipping 2810594.json as it is in news/data-detail2 folder\n",
      "Skipping 2810599.json as it is in news/data-detail2 folder\n",
      "Skipping 2810647.json as it is in news/data-detail2 folder\n",
      "Skipping 2810654.json as it is in news/data-detail2 folder\n",
      "Skipping 2810675.json as it is in news/data-detail2 folder\n",
      "Skipping 2810715.json as it is in news/data-detail2 folder\n",
      "Skipping 2810751.json as it is in news/data-detail2 folder\n",
      "Skipping 2810804.json as it is in news/data-detail2 folder\n",
      "Skipping 2810814.json as it is in news/data-detail2 folder\n",
      "Skipping 2810874.json as it is in news/data-detail2 folder\n",
      "Skipping 2810883.json as it is in news/data-detail2 folder\n",
      "Skipping 2810885.json as it is in news/data-detail2 folder\n",
      "Skipping 2810892.json as it is in news/data-detail2 folder\n",
      "Skipping 2810893.json as it is in news/data-detail2 folder\n",
      "Skipping 2810919.json as it is in news/data-detail2 folder\n",
      "Skipping 2810923.json as it is in news/data-detail2 folder\n",
      "Skipping 2810957.json as it is in news/data-detail2 folder\n",
      "Skipping 2810975.json as it is in news/data-detail2 folder\n",
      "Skipping 2810982.json as it is in news/data-detail2 folder\n",
      "Skipping 2810999.json as it is in news/data-detail2 folder\n",
      "Skipping 2811019.json as it is in news/data-detail2 folder\n",
      "Skipping 2811047.json as it is in news/data-detail2 folder\n",
      "Skipping 2811054.json as it is in news/data-detail2 folder\n",
      "Skipping 2811085.json as it is in news/data-detail2 folder\n",
      "Skipping 2811114.json as it is in news/data-detail2 folder\n",
      "Skipping 2811126.json as it is in news/data-detail2 folder\n",
      "Skipping 2811140.json as it is in news/data-detail2 folder\n",
      "Skipping 2811220.json as it is in news/data-detail2 folder\n",
      "Skipping 2811223.json as it is in news/data-detail2 folder\n",
      "Skipping 2811239.json as it is in news/data-detail2 folder\n",
      "Skipping 2811271.json as it is in news/data-detail2 folder\n",
      "Skipping 2811292.json as it is in news/data-detail2 folder\n",
      "Skipping 2811321.json as it is in news/data-detail2 folder\n",
      "Skipping 2811342.json as it is in news/data-detail2 folder\n",
      "Skipping 2811349.json as it is in news/data-detail2 folder\n",
      "Skipping 2811372.json as it is in news/data-detail2 folder\n",
      "Skipping 2811446.json as it is in news/data-detail2 folder\n",
      "Skipping 2811458.json as it is in news/data-detail2 folder\n",
      "Skipping 2811465.json as it is in news/data-detail2 folder\n",
      "Skipping 2811563.json as it is in news/data-detail2 folder\n",
      "Skipping 2811592.json as it is in news/data-detail2 folder\n",
      "Skipping 2811718.json as it is in news/data-detail2 folder\n",
      "Skipping 2811729.json as it is in news/data-detail2 folder\n",
      "Skipping 2811762.json as it is in news/data-detail2 folder\n",
      "Skipping 2811797.json as it is in news/data-detail2 folder\n",
      "Skipping 2811819.json as it is in news/data-detail2 folder\n",
      "Skipping 2811833.json as it is in news/data-detail2 folder\n",
      "Skipping 2811860.json as it is in news/data-detail2 folder\n",
      "Skipping 2811870.json as it is in news/data-detail2 folder\n",
      "Skipping 2811929.json as it is in news/data-detail2 folder\n",
      "Skipping 2811957.json as it is in news/data-detail2 folder\n",
      "Skipping 2811995.json as it is in news/data-detail2 folder\n",
      "Skipping 2812003.json as it is in news/data-detail2 folder\n",
      "Skipping 2812067.json as it is in news/data-detail2 folder\n",
      "Skipping 2812074.json as it is in news/data-detail2 folder\n",
      "Skipping 2812086.json as it is in news/data-detail2 folder\n",
      "Skipping 2812179.json as it is in news/data-detail2 folder\n",
      "Skipping 2812185.json as it is in news/data-detail2 folder\n",
      "Skipping 2812200.json as it is in news/data-detail2 folder\n",
      "Skipping 2812331.json as it is in news/data-detail2 folder\n",
      "Skipping 2812346.json as it is in news/data-detail2 folder\n",
      "Skipping 2812398.json as it is in news/data-detail2 folder\n",
      "Skipping 2812399.json as it is in news/data-detail2 folder\n",
      "Skipping 2812407.json as it is in news/data-detail2 folder\n",
      "Skipping 2812438.json as it is in news/data-detail2 folder\n",
      "Skipping 2812473.json as it is in news/data-detail2 folder\n",
      "Skipping 2812546.json as it is in news/data-detail2 folder\n",
      "Skipping 2812574.json as it is in news/data-detail2 folder\n",
      "Skipping 2812617.json as it is in news/data-detail2 folder\n",
      "Skipping 2812625.json as it is in news/data-detail2 folder\n",
      "Skipping 2812722.json as it is in news/data-detail2 folder\n",
      "Skipping 2812793.json as it is in news/data-detail2 folder\n",
      "Skipping 2812834.json as it is in news/data-detail2 folder\n",
      "Skipping 2812843.json as it is in news/data-detail2 folder\n",
      "Skipping 2812846.json as it is in news/data-detail2 folder\n",
      "Skipping 2812929.json as it is in news/data-detail2 folder\n",
      "Skipping 2812996.json as it is in news/data-detail2 folder\n",
      "Skipping 2813022.json as it is in news/data-detail2 folder\n",
      "Skipping 2813142.json as it is in news/data-detail2 folder\n",
      "Skipping 2813258.json as it is in news/data-detail2 folder\n",
      "Skipping 2813265.json as it is in news/data-detail2 folder\n",
      "Skipping 2813366.json as it is in news/data-detail2 folder\n",
      "Skipping 2813449.json as it is in news/data-detail2 folder\n",
      "Skipping 2813473.json as it is in news/data-detail2 folder\n",
      "Skipping 2813538.json as it is in news/data-detail2 folder\n",
      "Skipping 2813547.json as it is in news/data-detail2 folder\n",
      "Skipping 2813599.json as it is in news/data-detail2 folder\n",
      "Skipping 2813608.json as it is in news/data-detail2 folder\n",
      "Skipping 2813611.json as it is in news/data-detail2 folder\n",
      "Skipping 2813619.json as it is in news/data-detail2 folder\n",
      "Skipping 2813662.json as it is in news/data-detail2 folder\n",
      "Skipping 2813663.json as it is in news/data-detail2 folder\n",
      "Skipping 2813668.json as it is in news/data-detail2 folder\n",
      "Skipping 2813671.json as it is in news/data-detail2 folder\n",
      "Skipping 2813675.json as it is in news/data-detail2 folder\n",
      "Skipping 2813680.json as it is in news/data-detail2 folder\n",
      "Skipping 2813692.json as it is in news/data-detail2 folder\n",
      "Skipping 2813703.json as it is in news/data-detail2 folder\n",
      "Skipping 2813705.json as it is in news/data-detail2 folder\n",
      "Skipping 2813710.json as it is in news/data-detail2 folder\n",
      "Skipping 2813714.json as it is in news/data-detail2 folder\n",
      "Skipping 2813718.json as it is in news/data-detail2 folder\n",
      "Skipping 2813723.json as it is in news/data-detail2 folder\n",
      "Skipping 2813725.json as it is in news/data-detail2 folder\n",
      "Skipping 2813746.json as it is in news/data-detail2 folder\n",
      "Skipping 2813748.json as it is in news/data-detail2 folder\n",
      "Skipping 2813757.json as it is in news/data-detail2 folder\n",
      "Skipping 2813771.json as it is in news/data-detail2 folder\n",
      "Skipping 2813782.json as it is in news/data-detail2 folder\n",
      "Skipping 2813786.json as it is in news/data-detail2 folder\n",
      "Skipping 2813801.json as it is in news/data-detail2 folder\n",
      "Skipping 2813807.json as it is in news/data-detail2 folder\n",
      "Skipping 2813826.json as it is in news/data-detail2 folder\n",
      "Skipping 2813829.json as it is in news/data-detail2 folder\n",
      "Skipping 2813833.json as it is in news/data-detail2 folder\n",
      "Skipping 2813838.json as it is in news/data-detail2 folder\n",
      "Skipping 2813843.json as it is in news/data-detail2 folder\n",
      "Skipping 2813847.json as it is in news/data-detail2 folder\n",
      "Skipping 2813849.json as it is in news/data-detail2 folder\n",
      "Skipping 2813854.json as it is in news/data-detail2 folder\n",
      "Skipping 2813869.json as it is in news/data-detail2 folder\n",
      "Skipping 2813871.json as it is in news/data-detail2 folder\n",
      "Skipping 2813877.json as it is in news/data-detail2 folder\n",
      "Skipping 2813884.json as it is in news/data-detail2 folder\n",
      "Skipping 2813888.json as it is in news/data-detail2 folder\n",
      "Skipping 2813891.json as it is in news/data-detail2 folder\n",
      "Skipping 2813894.json as it is in news/data-detail2 folder\n",
      "Skipping 2813908.json as it is in news/data-detail2 folder\n",
      "Skipping 2813910.json as it is in news/data-detail2 folder\n",
      "Skipping 2813914.json as it is in news/data-detail2 folder\n",
      "Skipping 2813929.json as it is in news/data-detail2 folder\n",
      "Skipping 2813935.json as it is in news/data-detail2 folder\n",
      "Skipping 2813938.json as it is in news/data-detail2 folder\n",
      "Skipping 2813942.json as it is in news/data-detail2 folder\n",
      "Skipping 2813945.json as it is in news/data-detail2 folder\n",
      "Skipping 2813951.json as it is in news/data-detail2 folder\n",
      "Skipping 2813959.json as it is in news/data-detail2 folder\n",
      "Skipping 2813964.json as it is in news/data-detail2 folder\n",
      "Skipping 2813970.json as it is in news/data-detail2 folder\n",
      "Skipping 2813996.json as it is in news/data-detail2 folder\n",
      "Skipping 2814001.json as it is in news/data-detail2 folder\n",
      "Skipping 2814003.json as it is in news/data-detail2 folder\n",
      "Skipping 2814014.json as it is in news/data-detail2 folder\n",
      "Skipping 2814015.json as it is in news/data-detail2 folder\n",
      "Skipping 2814017.json as it is in news/data-detail2 folder\n",
      "Skipping 2814026.json as it is in news/data-detail2 folder\n",
      "Skipping 2814030.json as it is in news/data-detail2 folder\n",
      "Skipping 2814032.json as it is in news/data-detail2 folder\n",
      "Skipping 2814042.json as it is in news/data-detail2 folder\n",
      "Skipping 2814043.json as it is in news/data-detail2 folder\n",
      "Skipping 2814046.json as it is in news/data-detail2 folder\n",
      "Skipping 2814047.json as it is in news/data-detail2 folder\n",
      "Skipping 2814049.json as it is in news/data-detail2 folder\n",
      "Skipping 2814050.json as it is in news/data-detail2 folder\n",
      "Skipping 2814055.json as it is in news/data-detail2 folder\n",
      "Skipping 2814060.json as it is in news/data-detail2 folder\n",
      "Skipping 2814074.json as it is in news/data-detail2 folder\n",
      "Skipping 2814083.json as it is in news/data-detail2 folder\n",
      "Skipping 2814096.json as it is in news/data-detail2 folder\n",
      "Skipping 2814118.json as it is in news/data-detail2 folder\n",
      "Skipping 2814120.json as it is in news/data-detail2 folder\n",
      "Skipping 2814137.json as it is in news/data-detail2 folder\n",
      "Skipping 2814139.json as it is in news/data-detail2 folder\n",
      "Skipping 2814142.json as it is in news/data-detail2 folder\n",
      "Skipping 2814148.json as it is in news/data-detail2 folder\n",
      "Skipping 2814149.json as it is in news/data-detail2 folder\n",
      "Skipping 2814150.json as it is in news/data-detail2 folder\n",
      "Skipping 2814155.json as it is in news/data-detail2 folder\n",
      "Skipping 2814156.json as it is in news/data-detail2 folder\n",
      "Skipping 2814157.json as it is in news/data-detail2 folder\n",
      "Skipping 2814165.json as it is in news/data-detail2 folder\n",
      "Skipping 2814167.json as it is in news/data-detail2 folder\n",
      "Skipping 2814204.json as it is in news/data-detail2 folder\n",
      "Skipping 2814227.json as it is in news/data-detail2 folder\n",
      "Skipping 2814231.json as it is in news/data-detail2 folder\n",
      "Skipping 2814242.json as it is in news/data-detail2 folder\n",
      "Skipping 2814245.json as it is in news/data-detail2 folder\n",
      "Skipping 2814263.json as it is in news/data-detail2 folder\n",
      "Skipping 2814266.json as it is in news/data-detail2 folder\n",
      "Skipping 2814277.json as it is in news/data-detail2 folder\n",
      "Skipping 2814280.json as it is in news/data-detail2 folder\n",
      "Skipping 2814281.json as it is in news/data-detail2 folder\n",
      "Skipping 2814292.json as it is in news/data-detail2 folder\n",
      "Skipping 2814306.json as it is in news/data-detail2 folder\n",
      "Skipping 2814310.json as it is in news/data-detail2 folder\n",
      "Skipping 2814317.json as it is in news/data-detail2 folder\n",
      "Skipping 2814319.json as it is in news/data-detail2 folder\n",
      "Skipping 2814321.json as it is in news/data-detail2 folder\n",
      "Skipping 2814322.json as it is in news/data-detail2 folder\n",
      "Skipping 2814324.json as it is in news/data-detail2 folder\n",
      "Skipping 2814328.json as it is in news/data-detail2 folder\n",
      "Skipping 2814333.json as it is in news/data-detail2 folder\n",
      "Skipping 2814337.json as it is in news/data-detail2 folder\n",
      "Skipping 2814347.json as it is in news/data-detail2 folder\n",
      "Skipping 2814349.json as it is in news/data-detail2 folder\n",
      "Skipping 2814354.json as it is in news/data-detail2 folder\n",
      "Skipping 2814373.json as it is in news/data-detail2 folder\n",
      "Skipping 2814380.json as it is in news/data-detail2 folder\n",
      "Skipping 2814390.json as it is in news/data-detail2 folder\n",
      "Error processing chunk: name 'nlp' is not defined\n",
      "Processed 2814394.json in 635.17 ms\n",
      "Processed 2814400.json in 527.87 ms\n",
      "Processed 2814401.json in 1915.90 ms\n",
      "Processed 2814411.json in 969.80 ms\n",
      "Processed 2814414.json in 1397.03 ms\n",
      "Processed 2814433.json in 951.98 ms\n",
      "Processed 2814441.json in 820.93 ms\n",
      "Processed 2814462.json in 9291.72 ms\n",
      "Processed 2814472.json in 175.63 ms\n",
      "Processed 2814473.json in 273.42 ms\n",
      "Processed 2814483.json in 1550.32 ms\n",
      "Processed 2814508.json in 1164.54 ms\n",
      "Processed 2814511.json in 388.43 ms\n",
      "Processed 2814517.json in 1634.72 ms\n",
      "Processed 2814520.json in 1650.15 ms\n",
      "Processed 2814559.json in 4196.25 ms\n",
      "Processed 2814633.json in 9498.98 ms\n",
      "Processed 2814641.json in 2268.10 ms\n",
      "Processed 2814644.json in 425.65 ms\n",
      "Processed 2814647.json in 210.18 ms\n",
      "Processed 2814665.json in 1510.09 ms\n",
      "Processed 2814679.json in 920.13 ms\n",
      "Processed 2814695.json in 1312.79 ms\n",
      "Processed 2814698.json in 1419.56 ms\n",
      "Processed 2814705.json in 1051.70 ms\n",
      "Processed 2814730.json in 526.74 ms\n",
      "Processed 2814740.json in 2638.70 ms\n",
      "Processed 2814784.json in 596.31 ms\n",
      "Processed 2814834.json in 782.66 ms\n",
      "Processed 2814837.json in 601.86 ms\n",
      "Processed 2814838.json in 7279.80 ms\n",
      "Processed 2814841.json in 1420.05 ms\n",
      "Processed 2814856.json in 247.85 ms\n",
      "Processed 2814865.json in 1164.42 ms\n",
      "Processed 2814876.json in 670.75 ms\n",
      "Processed 2814886.json in 759.66 ms\n",
      "Processed 2814889.json in 1402.09 ms\n",
      "Processed 2814911.json in 1483.02 ms\n",
      "Processed 2814947.json in 1153.33 ms\n",
      "Processed 2814967.json in 1050.58 ms\n",
      "Processed 2814979.json in 2411.35 ms\n",
      "Processed 2815003.json in 1686.27 ms\n",
      "Processed 2815038.json in 2912.80 ms\n",
      "Processed 2815040.json in 719.99 ms\n",
      "Processed 2815042.json in 5363.35 ms\n",
      "Processed 2815043.json in 967.84 ms\n",
      "Processed 2815103.json in 1749.45 ms\n",
      "Processed 2815168.json in 532.97 ms\n",
      "Processed 2815178.json in 1098.64 ms\n",
      "Processed 2815190.json in 902.22 ms\n",
      "Processed 2815193.json in 564.62 ms\n",
      "Processed 2815269.json in 1083.06 ms\n",
      "Processed 2815274.json in 1133.15 ms\n",
      "Processed 2815277.json in 999.89 ms\n",
      "Processed 2815323.json in 1049.75 ms\n",
      "Processed 2815364.json in 1666.57 ms\n",
      "Processed 2815420.json in 616.63 ms\n",
      "Processed 2815469.json in 1949.69 ms\n",
      "Processed 2815556.json in 1799.80 ms\n",
      "Processed 2815576.json in 1333.21 ms\n",
      "Processed 2815613.json in 5936.49 ms\n",
      "Processed 2815624.json in 545.99 ms\n",
      "Processed 2815685.json in 1116.60 ms\n",
      "Processed 2815760.json in 7609.99 ms\n",
      "Processed 2815840.json in 987.95 ms\n",
      "Processed 2815897.json in 7214.57 ms\n",
      "Processed 2815935.json in 589.52 ms\n",
      "Processed 2816044.json in 1762.32 ms\n",
      "Processed 2816047.json in 1985.44 ms\n",
      "Processed 2816057.json in 2200.90 ms\n",
      "Processed 2816077.json in 2211.64 ms\n",
      "Processed 2816095.json in 858.60 ms\n",
      "Processed 2816109.json in 529.28 ms\n",
      "Processed 2816133.json in 1283.40 ms\n",
      "Processed 2816163.json in 1129.18 ms\n",
      "Processed 2816166.json in 900.02 ms\n",
      "Processed 2816174.json in 568.86 ms\n",
      "Processed 2816192.json in 1098.97 ms\n",
      "Processed 2816194.json in 1067.15 ms\n",
      "Processed 2816198.json in 898.08 ms\n",
      "Processed 2816216.json in 953.01 ms\n",
      "Processed 2816225.json in 980.30 ms\n",
      "Processed 2816256.json in 1169.32 ms\n",
      "Processed 2816260.json in 1465.84 ms\n",
      "Processed 2816280.json in 946.01 ms\n",
      "Processed 2816283.json in 584.30 ms\n",
      "Processed 2816287.json in 7265.35 ms\n",
      "Processed 2816290.json in 1299.86 ms\n",
      "Processed 2816320.json in 849.96 ms\n",
      "Processed 2816322.json in 1014.90 ms\n",
      "Processed 2816365.json in 1355.25 ms\n",
      "Processed 2816401.json in 1097.90 ms\n",
      "Processed 2816436.json in 745.37 ms\n",
      "Processed 2816444.json in 1234.49 ms\n",
      "Processed 2816456.json in 708.53 ms\n",
      "Processed 2816494.json in 1645.11 ms\n",
      "Processed 2816499.json in 766.89 ms\n",
      "Processed 2816506.json in 5478.58 ms\n",
      "Processed 2816507.json in 602.15 ms\n",
      "Processed 2816521.json in 735.20 ms\n",
      "Processed 2816531.json in 100.18 ms\n",
      "Processed 2816533.json in 981.98 ms\n",
      "Processed 2816542.json in 1031.00 ms\n",
      "Processed 2816549.json in 1101.71 ms\n",
      "Processed 2816588.json in 914.58 ms\n",
      "Processed 2816594.json in 1604.48 ms\n",
      "Processed 2816604.json in 798.43 ms\n",
      "Processed 2816607.json in 648.01 ms\n",
      "Processed 2816616.json in 798.49 ms\n",
      "Processed 2816636.json in 1602.13 ms\n",
      "Processed 2816639.json in 764.43 ms\n",
      "Processed 2816651.json in 1448.36 ms\n",
      "Processed 2816664.json in 1920.33 ms\n",
      "Processed 2816672.json in 1414.52 ms\n",
      "Processed 2816685.json in 1034.94 ms\n",
      "Processed 2816687.json in 1717.60 ms\n",
      "Processed 2816718.json in 163.19 ms\n",
      "Processed 2816732.json in 519.09 ms\n",
      "Processed 2816737.json in 5249.64 ms\n",
      "Processed 2816738.json in 433.25 ms\n",
      "Processed 2816739.json in 792.94 ms\n",
      "Processed 2816742.json in 1800.46 ms\n",
      "Processed 2816750.json in 637.93 ms\n",
      "Processed 2816761.json in 585.15 ms\n",
      "Processed 2816768.json in 1163.88 ms\n",
      "Processed 2816789.json in 1416.27 ms\n",
      "Processed 2816827.json in 1466.08 ms\n",
      "Processed 2816828.json in 1030.61 ms\n",
      "Processed 2816848.json in 1216.67 ms\n",
      "Processed 2816864.json in 318.22 ms\n",
      "Processed 2816893.json in 699.73 ms\n",
      "Processed 2816900.json in 969.51 ms\n",
      "Processed 2816918.json in 495.11 ms\n",
      "Processed 2816923.json in 5668.94 ms\n",
      "Processed 2816924.json in 501.80 ms\n",
      "Processed 2816932.json in 1664.88 ms\n",
      "Processed 2816942.json in 1165.64 ms\n",
      "Processed 2816951.json in 966.40 ms\n",
      "Processed 2816966.json in 1482.10 ms\n",
      "Processed 2816977.json in 814.77 ms\n",
      "Processed 2816991.json in 2999.55 ms\n",
      "Processed 2817017.json in 1232.49 ms\n",
      "Processed 2817036.json in 1382.79 ms\n",
      "Processed 2817062.json in 10631.98 ms\n",
      "Processed 2817072.json in 260.13 ms\n",
      "Processed 2817074.json in 1892.67 ms\n",
      "Processed 2817172.json in 4810.22 ms\n",
      "Processed 2817199.json in 7720.12 ms\n",
      "Processed 2817211.json in 146.89 ms\n",
      "Processed 2817213.json in 131.49 ms\n",
      "Processed 2817263.json in 1101.09 ms\n",
      "Processed 2817341.json in 637.15 ms\n",
      "Processed 2817389.json in 568.21 ms\n",
      "Processed 2817412.json in 4787.27 ms\n",
      "Processed 2817479.json in 1213.77 ms\n",
      "Processed 2817591.json in 920.24 ms\n",
      "Processed 2817599.json in 1400.33 ms\n",
      "Processed 2817606.json in 450.87 ms\n",
      "Processed 2817615.json in 2075.40 ms\n",
      "Processed 2817656.json in 5299.06 ms\n",
      "Processed 2817720.json in 1787.38 ms\n",
      "Processed 2817735.json in 1585.56 ms\n",
      "Processed 2817740.json in 1525.45 ms\n",
      "Processed 2817744.json in 1156.31 ms\n",
      "Processed 2817776.json in 1809.62 ms\n",
      "Processed 2817795.json in 674.06 ms\n",
      "Processed 2817890.json in 4293.89 ms\n",
      "Processed 2817914.json in 223.26 ms\n",
      "Processed 2817925.json in 1002.64 ms\n",
      "Processed 2817948.json in 397.07 ms\n",
      "Processed 2817961.json in 1117.55 ms\n",
      "Processed 2817975.json in 853.86 ms\n",
      "Processed 2817985.json in 960.00 ms\n",
      "Processed 2818011.json in 1049.49 ms\n",
      "Processed 2818030.json in 709.92 ms\n",
      "Processed 2818068.json in 1056.09 ms\n",
      "Processed 2818072.json in 1033.08 ms\n",
      "Processed 2818075.json in 876.37 ms\n",
      "Processed 2818081.json in 723.40 ms\n",
      "Processed 2818114.json in 1033.68 ms\n",
      "Processed 2818117.json in 379.80 ms\n",
      "Processed 2818130.json in 782.44 ms\n",
      "Processed 2818137.json in 1171.22 ms\n",
      "Processed 2818150.json in 395.95 ms\n",
      "Processed 2818158.json in 999.84 ms\n",
      "Processed 2818160.json in 683.13 ms\n",
      "Processed 2818169.json in 566.57 ms\n",
      "Processed 2818180.json in 916.57 ms\n",
      "Processed 2818212.json in 899.93 ms\n",
      "Processed 2818234.json in 838.28 ms\n",
      "Processed 2818240.json in 912.94 ms\n",
      "Processed 2818262.json in 1103.30 ms\n",
      "Processed 2818275.json in 834.19 ms\n",
      "Processed 2818279.json in 526.29 ms\n",
      "Processed 2818288.json in 783.20 ms\n",
      "Processed 2818290.json in 887.28 ms\n",
      "Processed 2818292.json in 6911.83 ms\n",
      "Processed 2818293.json in 516.55 ms\n",
      "Processed 2818305.json in 666.61 ms\n",
      "Processed 2818308.json in 183.48 ms\n",
      "Processed 2818318.json in 116.52 ms\n",
      "Error processing chunk: name 'nlp' is not defined\n",
      "Processed 2818320.json in 1477.21 ms\n",
      "Processed 2818336.json in 2257.16 ms\n",
      "Processed 2818353.json in 855.35 ms\n",
      "Processed 2818354.json in 879.23 ms\n",
      "Processed 2818357.json in 1148.90 ms\n",
      "Processed 2818369.json in 580.68 ms\n",
      "Processed 2818405.json in 872.44 ms\n",
      "Processed 2818410.json in 1283.82 ms\n",
      "Processed 2818423.json in 7724.58 ms\n",
      "Processed 2818427.json in 417.23 ms\n",
      "Processed 2818437.json in 581.17 ms\n",
      "Processed 2818438.json in 172.19 ms\n",
      "Processed 2818439.json in 133.21 ms\n",
      "Processed 2818444.json in 1277.95 ms\n",
      "Processed 2818473.json in 969.47 ms\n",
      "Processed 2818474.json in 1185.49 ms\n",
      "Processed 2818483.json in 1415.97 ms\n",
      "Processed 2818484.json in 465.61 ms\n",
      "Processed 2818485.json in 851.54 ms\n",
      "Processed 2818499.json in 583.13 ms\n",
      "Processed 2818530.json in 715.58 ms\n",
      "Processed 2818535.json in 547.88 ms\n",
      "Processed 2818562.json in 831.11 ms\n",
      "Processed 2818564.json in 7505.15 ms\n",
      "Processed 2818565.json in 1125.35 ms\n",
      "Processed 2818576.json in 2104.79 ms\n",
      "Processed 2818583.json in 128.80 ms\n",
      "Processed 2818585.json in 232.84 ms\n",
      "Processed 2818593.json in 1429.74 ms\n",
      "Processed 2818594.json in 1269.27 ms\n",
      "Processed 2818644.json in 466.53 ms\n",
      "Processed 2818664.json in 2016.72 ms\n",
      "Processed 2818671.json in 433.13 ms\n",
      "Processed 2818679.json in 2621.15 ms\n",
      "Processed 2818750.json in 12333.13 ms\n",
      "Processed 2818759.json in 1892.42 ms\n",
      "Processed 2818768.json in 170.12 ms\n",
      "Processed 2818805.json in 1302.65 ms\n",
      "Processed 2818852.json in 1986.93 ms\n",
      "Processed 2818888.json in 1396.99 ms\n",
      "Processed 2818893.json in 823.37 ms\n",
      "Processed 2818898.json in 1392.26 ms\n",
      "Processed 2818924.json in 1213.34 ms\n",
      "Processed 2818936.json in 6564.19 ms\n",
      "Processed 2818952.json in 7158.75 ms\n",
      "Processed 2818961.json in 1279.15 ms\n",
      "Processed 2819001.json in 1199.32 ms\n",
      "Processed 2819007.json in 1311.53 ms\n",
      "Processed 2819013.json in 1111.74 ms\n",
      "Processed 2819030.json in 1114.06 ms\n",
      "Processed 2819031.json in 865.71 ms\n",
      "Processed 2819044.json in 552.93 ms\n",
      "Processed 2819168.json in 1916.76 ms\n",
      "Processed 2819173.json in 4179.63 ms\n",
      "Processed 2819234.json in 2503.93 ms\n",
      "Processed 2819371.json in 1527.27 ms\n",
      "Processed 2819382.json in 3642.02 ms\n",
      "Processed 2819385.json in 879.44 ms\n",
      "Processed 2819394.json in 229.61 ms\n",
      "Processed 2819395.json in 896.42 ms\n",
      "Processed 2819435.json in 886.78 ms\n",
      "Processed 2819461.json in 981.60 ms\n",
      "Processed 2819572.json in 2707.73 ms\n",
      "Processed 2819688.json in 3886.08 ms\n",
      "Processed 2819710.json in 5413.35 ms\n",
      "Processed 2819795.json in 1424.92 ms\n",
      "Processed 2819866.json in 2200.03 ms\n",
      "Processed 2819919.json in 1692.39 ms\n",
      "Processed 2819983.json in 2195.64 ms\n",
      "Processed 2820299.json in 816.08 ms\n",
      "Processed 2820466.json in 950.47 ms\n",
      "Processed 2820467.json in 950.73 ms\n",
      "Processed 2820578.json in 1891.88 ms\n",
      "Processed 2820956.json in 2131.14 ms\n",
      "Processed 2821118.json in 1107.24 ms\n",
      "Processed 2821681.json in 1270.53 ms\n",
      "Processed 2821700.json in 3352.38 ms\n",
      "Processed 2821847.json in 2068.85 ms\n",
      "Processed 2822051.json in 1319.67 ms\n",
      "Processed 2822230.json in 1083.53 ms\n",
      "Processed 2822392.json in 583.01 ms\n",
      "Processed 2822490.json in 2349.77 ms\n",
      "Processed 2823135.json in 1099.80 ms\n",
      "Processed 2824502.json in 891.14 ms\n",
      "Processed 2827207.json in 2807.77 ms\n",
      "Processed 2828044.json in 6751.32 ms\n",
      "Processed 2828057.json in 151.94 ms\n",
      "Processed 2828629.json in 978.84 ms\n",
      "Processed 2831019.json in 2151.86 ms\n",
      "Processed 2831027.json in 201.00 ms\n",
      "Processed 2831171.json in 2546.57 ms\n",
      "Processed 2831296.json in 2933.60 ms\n",
      "Processed 2833704.json in 3750.69 ms\n"
     ]
    }
   ],
   "source": [
    "# text = \"Hello, my name is John Doe and I live in New York City.\"\n",
    "# result1 = perform_ner_based_on_language(text)\n",
    "\n",
    "import os;\n",
    "import json,codecs\n",
    "import time\n",
    "\n",
    "# list all files in news/data-detail folder\n",
    "folder_path = \"news/data-detail\"\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
    "\n",
    "# load all json files in news/data-detail folder by loop\n",
    "\n",
    "# data = []\n",
    "for json_file in json_files:\n",
    "    # skip loop if json_file is in news/data-detail2 folder\n",
    "    if os.path.exists(f\"news/data-detail2/{json_file}\"):\n",
    "        print(f\"Skipping {json_file} as it is in news/data-detail2 folder\")\n",
    "        continue\n",
    "\n",
    "    # i want to measure duration in ms of this process\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(os.path.join(folder_path, json_file), \"r\", encoding='utf-8') as f:\n",
    "        obj = json.load(f)\n",
    "    # process the data\n",
    "    data = {\n",
    "        \"title\": obj[\"title\"],\n",
    "        \"url\": obj[\"url\"],\n",
    "        # \"tags\": obj[\"tags\"],\n",
    "        \"publish_date\": thai_date_to_standard(obj[\"publish_date\"]),\n",
    "        \"content\": obj[\"ner_obj\"][\"original_message\"]\n",
    "    }\n",
    "    # chunk data into smaller pieces not more than 256 characters\n",
    "    chunks = [data[\"content\"][i:i + 256] for i in range(0, len(data[\"content\"]), 256)]\n",
    "    # loop through chunks and NER\n",
    "    result = { \"entities\": [] }\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            t = perform_ner_based_on_language(chunk)\n",
    "            new_entities = t[\"entities\"]\n",
    "            result[\"entities\"].extend(new_entities)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            continue\n",
    "\n",
    "    # LOCATIONS\n",
    "    locations = []\n",
    "    for entity in result[\"entities\"]:\n",
    "        if entity[\"type\"] == \"LOCATION\" and entity[\"value\"].strip() != \"\": \n",
    "            locations.append(entity[\"value\"].strip())\n",
    "    location_string = \" | \".join(locations)\n",
    "    province = \" | \".join(location_detection(location_string))\n",
    "    data[\"location\"] = province\n",
    "\n",
    "    # Measure duration\n",
    "    duration_ms = (time.time() - start_time) * 1000  # convert to ms\n",
    "    print(f\"Processed {json_file} in {duration_ms:.2f} ms\")\n",
    "\n",
    "    data[\"duration_ms\"] = duration_ms\n",
    "\n",
    "    # Save the processed data to news/data-detail folder\n",
    "    with codecs.open(f\"news/data-detail2/{json_file}\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # print(result)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 862659.58 ms\n"
     ]
    }
   ],
   "source": [
    "import os;\n",
    "import json,codecs\n",
    "import time\n",
    "\n",
    "# list all files in news/data-detail folder\n",
    "folder_path = \"news/data-detail2\"\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
    "\n",
    "# load all json files in news/data-detail folder by loop\n",
    "\n",
    "# data = []\n",
    "total_duration_ms = 0\n",
    "for json_file in json_files:\n",
    "    with open(os.path.join(folder_path, json_file), \"r\", encoding='utf-8') as f:\n",
    "        obj = json.load(f)\n",
    "\n",
    "    # calculate sum of duration_ms in all processed files\n",
    "    total_duration_ms += obj.get(\"duration_ms\", 0)\n",
    "    \n",
    "print(f\"Total processing time: {total_duration_ms:.2f} ms\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promhod-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
